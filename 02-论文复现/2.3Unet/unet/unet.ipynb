{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "img = cv2.imread('VOCDatasets/SegmentationClassPNG/CHNCXR_0019_0.png',cv2.IMREAD_GRAYSCALE)\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(img, cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.title('Image')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过网盘分享的文件：LungDataset.zip\n",
    "链接: https://pan.baidu.com/s/18Vy_cH0DfiXhjJLMC-YrJw?pwd=atqj 提取码: atqj \n",
    "--来自百度网盘超级会员v5的分享"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 导入相关库\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torchvision.transforms.functional as TF\n",
    "import torchvision.transforms as transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 创建随机种子\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 配置参数\n",
    "class Config:\n",
    "    # 数据集路径\n",
    "    DATA_ROOT = Path('LungDataset')  # 修改为处理后的数据集路径\n",
    "    IMAGES_DIR = DATA_ROOT / 'images'\n",
    "    MASKS_DIR = DATA_ROOT / 'masks'\n",
    "    \n",
    "    # 训练参数\n",
    "    BATCH_SIZE = 4\n",
    "    LEARNING_RATE = 1e-4\n",
    "    NUM_EPOCHS = 50\n",
    "    DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    IMG_SIZE = 512  # 调整图像大小为512x512\n",
    "    SEED = 42\n",
    "    VAL_SPLIT = 0.2  # 20%的数据用于验证\n",
    "    \n",
    "    # 模型参数\n",
    "    IN_CHANNELS = 1  # 灰度图像\n",
    "    OUT_CHANNELS = 1  # 二元分割\n",
    "    FEATURES = [64, 128, 256, 512]  # UNet特征通道数\n",
    "    \n",
    "    # 保存路径\n",
    "    CHECKPOINTS_DIR = Path('checkpoints')\n",
    "    LOGS_DIR = Path('logs')\n",
    "    RESULTS_DIR = Path('results')\n",
    "    \n",
    "    # 创建必要的目录\n",
    "    def create_directories(self):\n",
    "        os.makedirs(self.CHECKPOINTS_DIR, exist_ok=True)\n",
    "        os.makedirs(self.LOGS_DIR, exist_ok=True)\n",
    "        os.makedirs(self.RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "cfg = Config()\n",
    "cfg.create_directories()\n",
    "set_seed(cfg.SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义U-Net模型的双卷积块\n",
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(DoubleConv, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels=1, out_channels=1, features=None):\n",
    "        super(UNet, self).__init__()\n",
    "        if features is None:\n",
    "            features = [64, 128, 256, 512]\n",
    "        \n",
    "        self.downs = nn.ModuleList()\n",
    "        self.ups = nn.ModuleList()\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # 下采样部分\n",
    "        for feature in features:\n",
    "            self.downs.append(DoubleConv(in_channels, feature))\n",
    "            in_channels = feature\n",
    "        \n",
    "        # 上采样部分\n",
    "        for feature in reversed(features):\n",
    "            self.ups.append(\n",
    "                nn.ConvTranspose2d(feature*2, feature, kernel_size=2, stride=2)\n",
    "            )\n",
    "            self.ups.append(DoubleConv(feature*2, feature))\n",
    "        \n",
    "        self.bottleneck = DoubleConv(features[-1], features[-1]*2)\n",
    "        self.final_conv = nn.Conv2d(features[0], out_channels, kernel_size=1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        skip_connections = []\n",
    "        \n",
    "        # 下采样路径\n",
    "        for down in self.downs:\n",
    "            x = down(x)\n",
    "            skip_connections.append(x)\n",
    "            x = self.pool(x)\n",
    "        \n",
    "        x = self.bottleneck(x)\n",
    "        skip_connections = skip_connections[::-1]  # 反转列表\n",
    "        \n",
    "        # 上采样路径\n",
    "        for idx in range(0, len(self.ups), 2):\n",
    "            x = self.ups[idx](x)  # 转置卷积\n",
    "            skip_connection = skip_connections[idx//2]\n",
    "            \n",
    "            # 处理尺寸不匹配的情况\n",
    "            if x.shape != skip_connection.shape:\n",
    "                x = TF.resize(x, size=skip_connection.shape[2:])\n",
    "            \n",
    "            concat_skip = torch.cat((skip_connection, x), dim=1)\n",
    "            x = self.ups[idx+1](concat_skip)  # 双卷积\n",
    "        \n",
    "        x = self.final_conv(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自定义数据增强转换\n",
    "class RandomFlip:\n",
    "    def __init__(self, p=0.5):\n",
    "        self.p = p\n",
    "    \n",
    "    def __call__(self, img, mask):\n",
    "        if random.random() < self.p:\n",
    "            img = TF.hflip(img)\n",
    "            mask = TF.hflip(mask)\n",
    "        if random.random() < self.p:\n",
    "            img = TF.vflip(img)\n",
    "            mask = TF.vflip(mask)\n",
    "        return img, mask\n",
    "\n",
    "class RandomRotation:\n",
    "    def __init__(self, p=0.5):\n",
    "        self.p = p\n",
    "        self.angles = [0, 90, 180, 270]\n",
    "    \n",
    "    def __call__(self, img, mask):\n",
    "        if random.random() < self.p:\n",
    "            angle = random.choice(self.angles)\n",
    "            img = TF.rotate(img, angle)\n",
    "            mask = TF.rotate(mask, angle)\n",
    "        return img, mask\n",
    "\n",
    "class RandomBrightnessContrast:\n",
    "    def __init__(self, p=0.2, brightness=0.2, contrast=0.2):\n",
    "        self.p = p\n",
    "        self.brightness = brightness\n",
    "        self.contrast = contrast\n",
    "    \n",
    "    def __call__(self, img, mask):\n",
    "        if random.random() < self.p:\n",
    "            brightness_factor = random.uniform(1-self.brightness, 1+self.brightness)\n",
    "            img = TF.adjust_brightness(img, brightness_factor)\n",
    "        \n",
    "        if random.random() < self.p:\n",
    "            contrast_factor = random.uniform(1-self.contrast, 1+self.contrast)\n",
    "            img = TF.adjust_contrast(img, contrast_factor)\n",
    "            \n",
    "        return img, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LungSegmentationDataset(Dataset):\n",
    "    def __init__(self, images_paths, phase=\"train\"):\n",
    "        self.images_paths = images_paths\n",
    "        self.phase = phase\n",
    "        \n",
    "        # 基本转换\n",
    "        self.resize = transforms.Resize((cfg.IMG_SIZE, cfg.IMG_SIZE), \n",
    "                                       interpolation=transforms.InterpolationMode.NEAREST)\n",
    "        self.to_tensor = transforms.ToTensor()\n",
    "        self.normalize = transforms.Normalize((0.5,), (0.5,))\n",
    "        \n",
    "        # 训练时的数据增强\n",
    "        if phase == \"train\":\n",
    "            self.augmentations = [\n",
    "                RandomFlip(p=0.5),\n",
    "                RandomRotation(p=0.5),\n",
    "                RandomBrightnessContrast(p=0.2)\n",
    "            ]\n",
    "        else:\n",
    "            self.augmentations = []\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # 获取图像路径\n",
    "        img_path = self.images_paths[idx]\n",
    "        \n",
    "        # 构建对应的掩码路径（文件名相同，目录不同）\n",
    "        mask_path = cfg.MASKS_DIR / img_path.name\n",
    "        \n",
    "        # 读取图像和掩码\n",
    "        image = Image.open(str(img_path)).convert(\"L\")  # 转换为灰度图\n",
    "        mask = Image.open(str(mask_path)).convert(\"L\")  # 转换为灰度图\n",
    "        \n",
    "        # 调整大小\n",
    "        image = self.resize(image)\n",
    "        mask = self.resize(mask)\n",
    "        \n",
    "        # 应用数据增强（如果是训练阶段）\n",
    "        for aug in self.augmentations:\n",
    "            image, mask = aug(image, mask)\n",
    "        \n",
    "        # 转换为张量\n",
    "        image = self.to_tensor(image)\n",
    "        \n",
    "        # 归一化图像\n",
    "        image = self.normalize(image)\n",
    "        \n",
    "        # 处理掩码（二值化并归一化）\n",
    "        mask = torch.from_numpy(np.array(mask))\n",
    "        mask = (mask > 127).float()\n",
    "        mask = mask.unsqueeze(0)  # 添加通道维度\n",
    "        \n",
    "        return image, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataloaders():\n",
    "    # 获取所有图像文件路径\n",
    "    image_files = sorted(list(cfg.IMAGES_DIR.glob(\"*.png\")))\n",
    "    \n",
    "    # 划分训练集和验证集\n",
    "    train_files, val_files = train_test_split(\n",
    "        image_files, test_size=cfg.VAL_SPLIT, random_state=cfg.SEED\n",
    "    )\n",
    "    \n",
    "    # 创建数据集\n",
    "    train_dataset = LungSegmentationDataset(\n",
    "        train_files, phase=\"train\"\n",
    "    )\n",
    "    \n",
    "    val_dataset = LungSegmentationDataset(\n",
    "        val_files, phase=\"val\"\n",
    "    )\n",
    "    \n",
    "    # 创建数据加载器\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, batch_size=cfg.BATCH_SIZE, \n",
    "        shuffle=True, num_workers=0, pin_memory=True\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, batch_size=cfg.BATCH_SIZE, \n",
    "        shuffle=False, num_workers=0, pin_memory=True\n",
    "    )\n",
    "    \n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练一个epoch\n",
    "def train_epoch(model, loader, optimizer, loss_fn, device):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for batch_idx, (images, masks) in enumerate(tqdm(loader, desc=\"训练中\")):\n",
    "        images = images.to(device)\n",
    "        masks = masks.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(images)\n",
    "        \n",
    "        loss = loss_fn(predictions, masks)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    return epoch_loss / len(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 验证一个epoch\n",
    "def val_epoch(model, loader, loss_fn, device):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    dice_scores = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (images, masks) in enumerate(tqdm(loader, desc=\"验证中\")):\n",
    "            images = images.to(device)\n",
    "            masks = masks.to(device)\n",
    "            \n",
    "            predictions = model(images)\n",
    "            loss = loss_fn(predictions, masks)\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "            # 计算Dice系数\n",
    "            preds = (predictions > 0.5).float()\n",
    "            batch_dice = (2.0 * (preds * masks).sum()) / (preds.sum() + masks.sum() + 1e-8)\n",
    "            dice_scores.append(batch_dice.item())\n",
    "    \n",
    "    return epoch_loss / len(loader), np.mean(dice_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dice损失函数\n",
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self, smooth=1.0):\n",
    "        super(DiceLoss, self).__init__()\n",
    "        self.smooth = smooth\n",
    "    \n",
    "    def forward(self, predictions, targets):\n",
    "        # 展平预测和目标\n",
    "        predictions = predictions.view(-1)\n",
    "        targets = targets.view(-1)\n",
    "        \n",
    "        # 计算交集\n",
    "        intersection = (predictions * targets).sum()\n",
    "        \n",
    "        # 计算Dice系数: 2*|X∩Y| / (|X|+|Y|)\n",
    "        dice_score = (2.0 * intersection + self.smooth) / (\n",
    "            predictions.sum() + targets.sum() + self.smooth\n",
    "        )\n",
    "        \n",
    "        # 返回Dice损失\n",
    "        return 1.0 - dice_score\n",
    "\n",
    "# 组合损失函数：BCE + Dice\n",
    "class BCEDiceLoss(nn.Module):\n",
    "    def __init__(self, bce_weight=0.5, dice_weight=0.5):\n",
    "        super(BCEDiceLoss, self).__init__()\n",
    "        self.bce = nn.BCELoss()\n",
    "        self.dice = DiceLoss()\n",
    "        self.bce_weight = bce_weight\n",
    "        self.dice_weight = dice_weight\n",
    "    \n",
    "    def forward(self, predictions, targets):\n",
    "        bce_loss = self.bce(predictions, targets)\n",
    "        dice_loss = self.dice(predictions, targets)\n",
    "        return self.bce_weight * bce_loss + self.dice_weight * dice_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存最佳模型和最后一个模型\n",
    "def save_model(model, filename):\n",
    "    filepath = cfg.CHECKPOINTS_DIR / filename\n",
    "    torch.save(model.state_dict(), filepath)\n",
    "    print(f\"模型已保存至 {filepath}\")\n",
    "\n",
    "# 可视化预测结果\n",
    "def visualize_predictions(model, loader, device, num_samples=4):\n",
    "    model.eval()\n",
    "    images, masks, preds = [], [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (image, mask) in enumerate(loader):\n",
    "            if batch_idx >= num_samples:\n",
    "                break\n",
    "                \n",
    "            image = image.to(device)\n",
    "            pred = model(image)\n",
    "            \n",
    "            # 转换为numpy数组以便可视化\n",
    "            image = image.cpu().numpy()[0, 0]\n",
    "            mask = mask.cpu().numpy()[0, 0]\n",
    "            pred = (pred > 0.5).float().cpu().numpy()[0, 0]\n",
    "            \n",
    "            images.append(image)\n",
    "            masks.append(mask)\n",
    "            preds.append(pred)\n",
    "    \n",
    "    # 创建可视化结果\n",
    "    plt.figure(figsize=(12, 4 * num_samples))\n",
    "    for i in range(len(images)):\n",
    "        # 原始图像\n",
    "        plt.subplot(num_samples, 3, i*3 + 1)\n",
    "        plt.imshow(images[i], cmap='gray')\n",
    "        plt.title('原始图像')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        # 真实掩码\n",
    "        plt.subplot(num_samples, 3, i*3 + 2)\n",
    "        plt.imshow(masks[i], cmap='gray')\n",
    "        plt.title('真实掩码')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        # 预测掩码\n",
    "        plt.subplot(num_samples, 3, i*3 + 3)\n",
    "        plt.imshow(preds[i], cmap='gray')\n",
    "        plt.title('预测掩码')\n",
    "        plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(cfg.RESULTS_DIR / 'predictions.png')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 主训练函数\n",
    "def train_model():\n",
    "    # 准备数据加载器\n",
    "    train_loader, val_loader = prepare_dataloaders()\n",
    "    \n",
    "    # 创建模型\n",
    "    model = UNet(\n",
    "        in_channels=cfg.IN_CHANNELS,\n",
    "        out_channels=cfg.OUT_CHANNELS,\n",
    "        features=cfg.FEATURES\n",
    "    )\n",
    "    model = model.to(cfg.DEVICE)\n",
    "    \n",
    "    # 定义优化器和损失函数\n",
    "    optimizer = optim.Adam(model.parameters(), lr=cfg.LEARNING_RATE)\n",
    "    loss_fn = BCEDiceLoss()\n",
    "    \n",
    "    # 学习率调度器\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.1, patience=5, verbose=True\n",
    "    )\n",
    "    \n",
    "    # TensorBoard写入器\n",
    "    writer = SummaryWriter(log_dir=cfg.LOGS_DIR)\n",
    "    \n",
    "    # 训练循环\n",
    "    best_val_loss = float('inf')\n",
    "    for epoch in range(cfg.NUM_EPOCHS):\n",
    "        print(f\"Epoch {epoch+1}/{cfg.NUM_EPOCHS}\")\n",
    "        \n",
    "        # 训练\n",
    "        train_loss = train_epoch(model, train_loader, optimizer, loss_fn, cfg.DEVICE)\n",
    "        \n",
    "        # 验证\n",
    "        val_loss, val_dice = val_epoch(model, val_loader, loss_fn, cfg.DEVICE)\n",
    "        \n",
    "        # 学习率调度\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        # 记录到TensorBoard\n",
    "        writer.add_scalar(\"Loss/train\", train_loss, epoch)\n",
    "        writer.add_scalar(\"Loss/val\", val_loss, epoch)\n",
    "        writer.add_scalar(\"Dice/val\", val_dice, epoch)\n",
    "        \n",
    "        # 打印结果\n",
    "        print(f\"训练损失: {train_loss:.4f}, 验证损失: {val_loss:.4f}, 验证Dice: {val_dice:.4f}\")\n",
    "        \n",
    "        # 保存最佳模型\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            save_model(model, \"best_model.pth\")\n",
    "        \n",
    "        # 每10个epoch可视化一次预测结果\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            visualize_predictions(model, val_loader, cfg.DEVICE)\n",
    "    \n",
    "    # 保存最后一个模型\n",
    "    save_model(model, \"last_model.pth\")\n",
    "    writer.close()\n",
    "    \n",
    "    return model\n",
    "\n",
    "# 测试函数\n",
    "def test_model(model_path):\n",
    "    # 加载最佳模型\n",
    "    model = UNet(\n",
    "        in_channels=cfg.IN_CHANNELS,\n",
    "        out_channels=cfg.OUT_CHANNELS,\n",
    "        features=cfg.FEATURES\n",
    "    )\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model = model.to(cfg.DEVICE)\n",
    "    \n",
    "    # 准备数据加载器\n",
    "    _, val_loader = prepare_dataloaders()\n",
    "    \n",
    "    # 评估模型\n",
    "    loss_fn = BCEDiceLoss()\n",
    "    val_loss, val_dice = val_epoch(model, val_loader, loss_fn, cfg.DEVICE)\n",
    "    print(f\"测试损失: {val_loss:.4f}, 测试Dice: {val_dice:.4f}\")\n",
    "    \n",
    "    # 可视化预测结果\n",
    "    visualize_predictions(model, val_loader, cfg.DEVICE, num_samples=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # 检查处理后的数据集是否存在\n",
    "    if not cfg.DATA_ROOT.exists() or not cfg.IMAGES_DIR.exists() or not cfg.MASKS_DIR.exists():\n",
    "        print(f\"警告: 处理后的数据集不存在，请先运行 preprocess_dataset.py 脚本处理数据集!\")\n",
    "        return\n",
    "        \n",
    "    print(f\"使用设备: {cfg.DEVICE}\")\n",
    "    print(f\"数据集路径: {cfg.DATA_ROOT}\")\n",
    "    print(f\"图像数量: {len(list(cfg.IMAGES_DIR.glob('*.png')))}\")\n",
    "    print(\"开始训练...\")\n",
    "    \n",
    "    # 训练模型\n",
    "    model = train_model()\n",
    "    \n",
    "    # 测试最佳模型\n",
    "    best_model_path = cfg.CHECKPOINTS_DIR / \"best_model.pth\"\n",
    "    print(\"\\n评估最佳模型...\")\n",
    "    test_model(best_model_path)\n",
    "    \n",
    "    print(\"完成!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
